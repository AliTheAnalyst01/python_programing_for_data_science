{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command imports the class Counter from the collections library. Counter is a very useful tool for data scientists; it can count the number of times items appear in collections such as lists. For example, in the code below we will create a list of marriage ages. Using Counter we can quickly count the number of times each unique age appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22, 2), (25, 2), (24, 2), (30, 1), (26, 1), (35, 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "marriage_ages = [22, 22, 25, 25, 30, 24, 26, 24, 35]  # create a list\n",
    "value_counts = Counter(marriage_ages)  # apply the counter functionality\n",
    "print(value_counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "def add_two_numbers(x, y):  # function header\n",
    "    \"\"\"\n",
    "    Takes in two numbers and returns the sum\n",
    "    parameters\n",
    "        x : str\n",
    "            first number\n",
    "        y : str\n",
    "            second number\n",
    "    returns\n",
    "        x+y\n",
    "    \"\"\"\n",
    "    z = x + y\n",
    "    return z  # function return\n",
    "print(add_two_numbers(100,5))  # function  call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---0---\n",
      "[1, 5, 3, 6, 4]\n",
      "---1---\n",
      "48\n",
      "---2---\n",
      "12\n",
      "---3---\n",
      "12\n",
      "---4---\n",
      "[10, 12]\n",
      "---5---\n",
      "[3, 6, 4]\n",
      "---6---\n",
      "False\n",
      "True\n",
      "---7---\n",
      "[1, 5, 3, 6, 4, 7, 10, 12, 44]\n",
      "---8---\n",
      "[1, 5, 3, 6, 4, 7, 10, 12, 44, 100, 200]\n",
      "---9---\n",
      "[1, 5, 3, 6, 100, 7, 10, 12, 44, 100, 200]\n",
      "---10---\n",
      "[1, 5, 3, 6, 100, 1000, 7, 10, 12, 44, 100, 200]\n"
     ]
    }
   ],
   "source": [
    "# This creates the list\n",
    "depths = [1, 5, 3, 6, 4, 7, 10, 12]\n",
    "\n",
    "# This outputs the first 5 elements. No number before the : implies 0\n",
    "first_5_depths = depths[:5]\n",
    "\n",
    "print(\"---0---\")\n",
    "print(first_5_depths)\n",
    "\n",
    "# You can easily sum\n",
    "print(\"---1---\")\n",
    "print(sum(depths))\n",
    "\n",
    "# And take the max\n",
    "print(\"---2---\")\n",
    "print(max(depths))\n",
    "\n",
    "# Slicing with a negative starts from the end, so this returns the last element\n",
    "print(\"---3---\")\n",
    "print(depths[-1])\n",
    "\n",
    "# This returns the end of the list starting from the second to the end\n",
    "# Nothing after the : implies the end of the list\n",
    "print(\"---4---\")\n",
    "print(depths[-2:])\n",
    "\n",
    "# This returns the second, third, and forth elements\n",
    "# Remember counting starts at zero!\n",
    "print(\"---5---\")\n",
    "print(depths[2:5])\n",
    "\n",
    "# These commands check if a value is contained in the list\n",
    "print(\"---6---\")\n",
    "print(22 in depths)\n",
    "print(1 in depths)\n",
    "\n",
    "# This is how you add another value to the end of your list\n",
    "depths.append(44)\n",
    "print(\"---7---\")\n",
    "print(depths)\n",
    "\n",
    "# You can extend a list with another list\n",
    "depths.extend([100, 200])\n",
    "print(\"---8---\")\n",
    "print(depths)\n",
    "\n",
    "# You can also modify a value\n",
    "# This replaces the 4th value with 100\n",
    "depths[4] = 100\n",
    "print(\"---9---\")\n",
    "print(depths)\n",
    "\n",
    "# Or you can do insert to accomplish the same thing\n",
    "depths.insert(5, 1000)\n",
    "print(\"---10---\")\n",
    "print(depths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---0---\n",
      "22\n",
      "---1---\n",
      "True\n",
      "---2---\n",
      "False\n",
      "---3---\n",
      "22\n",
      "---4---\n",
      "1\n",
      "---5---\n",
      "dict_keys(['age', 'birth_year', 'name', 'siblings'])\n",
      "---6---\n",
      "dict_values([22, 1999, 'jack', ['jill', 'jen']])\n",
      "---7---\n",
      "dict_items([('age', 22), ('birth_year', 1999), ('name', 'jack'), ('siblings', ['jill', 'jen'])])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary.\n",
    "# Keys are first then a : then the value\n",
    "my_dict = {\"age\": 22, \"birth_year\": 1999, \"name\": \"jack\", \"siblings\": [\"jill\", \"jen\"]}\n",
    "\n",
    "# Get the value for the key age\n",
    "print(\"---0---\")\n",
    "print(my_dict['age'])\n",
    "\n",
    "# Check is age is a key\n",
    "print(\"---1---\")\n",
    "print('age' in my_dict)\n",
    "\n",
    "# Check is company is a key\n",
    "print(\"---2---\")\n",
    "print('company' in my_dict)\n",
    "\n",
    "# Get the value for they key age\n",
    "print(\"---3---\")\n",
    "print(my_dict.get('age'))\n",
    "\n",
    "# Get the value for they key company\n",
    "# If it doesn't exsist, return 1\n",
    "print(\"---4---\")\n",
    "print(my_dict.get('company', 1))\n",
    "\n",
    "# Return all the keys\n",
    "print(\"---5---\")\n",
    "print(my_dict.keys())\n",
    "\n",
    "# Return all the values\n",
    "print(\"---6---\")\n",
    "print(my_dict.values())\n",
    "\n",
    "# Return all the key, value pairs\n",
    "print(\"---7---\")\n",
    "print(my_dict.items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sets in Python \n",
    "Sets are another useful data type. Sets are an unordered collection of unique elements, which means any duplicates are automatically removed. Sets allow you to do operations like union, intersection, and difference. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---0---\n",
      "{1, 2}\n",
      "---1---\n",
      "{1, 2, 3, 4}\n",
      "---1---\n",
      "{1, 2}\n",
      "---2---\n",
      "{1, 2, 3, 4}\n",
      "---3---\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "my_set = set()\n",
    "my_set.add(1)\n",
    "my_set.add(2)\n",
    "my_set.add(1)\n",
    "# Note that the set only contains a single 1 value\n",
    "print(\"---0---\")\n",
    "print(my_set)\n",
    "\n",
    "my_set2 = set()\n",
    "my_set2.add(1)\n",
    "my_set2.add(2)\n",
    "my_set2.add(3)\n",
    "my_set2.add(4)\n",
    "print(\"---1---\")\n",
    "print(my_set2)\n",
    "\n",
    "# Prints the overlap\n",
    "print(\"---1---\")\n",
    "print(my_set.intersection(my_set2))\n",
    "print(\"---2---\")\n",
    "\n",
    "# Prints the combination\n",
    "print(my_set.union(my_set2))\n",
    "\n",
    "# Prints the difference (those in my_set but not my_set2)\n",
    "print(\"---3---\")\n",
    "print(my_set.difference(my_set2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control structures in Python \n",
    "The if-else construct #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Older than 40\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def age_check(age):\n",
    "\n",
    "    if age > 40:  # if age greater than 40, print \"Older than 40\"\n",
    "        print(\"Older than 40\")\n",
    "    elif age > 30 and age <= 40: # if age greater than 30 and less than or equal to 40, print \"Between 30 and 40\"\n",
    "        print(\"Between 30 and 40\")\n",
    "    else:  # if neither of the previous conditions are met, print \"Other\"\n",
    "        print(\"Other\")\n",
    "\n",
    "print(age_check(41))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The for construct \n",
    "Loops allow you to iterate over an iterable. That’s not a very helpful definition, so let’s consider the most common use case, lists. A loop allows you to iterate over a list or other data types that also allow iteration.\n",
    "\n",
    "You can contain your iterable in the enumerate() command to add a counter to your loop. This is useful if you want to loop over a list of values while still having access to the iterable index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Value: tyler\n",
      "Index: 1\n",
      "Value: karen\n",
      "Index: 2\n",
      "Value: jill\n"
     ]
    }
   ],
   "source": [
    "names = ['tyler', 'karen', 'jill']   # list containing names\n",
    "\n",
    "for i, name in enumerate(names):     # iterating over names\n",
    "    print(\"Index: {0}\".format(i))    # printing index number\n",
    "    print(\"Value: {0}\".format(name)) # print the value at the index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The zip function \n",
    "Lastly, a slightly more complicated function: zip. You can do a lot of useful things with zip, but here are 2 common use cases:\n",
    "\n",
    "Combining two lists into a list of tuples\n",
    "Breaking a tuple into two lists\n",
    "Combining two lists into a tuple \n",
    "The zip actually returns a generator, so we have to wrap it in list() to print it. This would not be necessary if you wanted to loop over it though, because generators are iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'x'), (2, 'y'), (3, 'z')]\n"
     ]
    }
   ],
   "source": [
    "list_1 = [1, 2, 3]  # create your first list\n",
    "list_2 = ['x', 'y', 'z']  # create your second list\n",
    "\n",
    "print(list(zip(list_1, list_2)))  #combine and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', 'y', 'z')\n",
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "pairs = [('x', 1), ('y', 2), ('z', 3)]  # a list of tuples\n",
    "letters, numbers = zip(*pairs)  # break into two lists\n",
    "\n",
    "print(letters)  # print the first values of the tuples\n",
    "print(numbers)  # print the second values of the tuples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling arrays with NumPy \n",
    "Single dimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0--\n",
      "[ 5 10 15 20 25 30]\n",
      "--1--\n",
      "8.539125638299666\n",
      "--2--\n",
      "30\n",
      "--3--\n",
      "[ 25 100 225 400 625 900]\n",
      "--4--\n",
      "[10 20 30 40 50 60]\n",
      "--5--\n",
      "2275\n",
      "--6--\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This creates our array\n",
    "np_array = np.array([5, 10, 15, 20, 25, 30])\n",
    "print(\"--0--\")\n",
    "\n",
    "# Gets the unique values\n",
    "print(np.unique(np_array))\n",
    "print(\"--1--\")\n",
    "\n",
    "# Calculates the standard deviation\n",
    "print(np.std(np_array))\n",
    "print(\"--2--\")\n",
    "\n",
    "# Calculates the maximum\n",
    "print(np_array.max())\n",
    "print(\"--3--\")\n",
    "\n",
    "# Squares each value in the array\n",
    "print(np_array ** 2)\n",
    "print(\"--4--\")\n",
    "\n",
    "# Adds the arrays together element wise\n",
    "print(np_array + np_array)\n",
    "print(\"--5--\")\n",
    "\n",
    "# The sum of the squares of the elements\n",
    "print(np.sum(np_array ** 2))\n",
    "print(\"--6--\")\n",
    "\n",
    "# Gives you the shape: (rows, columns)\n",
    "print(np_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0--\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "--1--\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "--3--\n",
      "(2, 3)\n",
      "--4--\n",
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create 2d array\n",
    "print(\"--0--\")\n",
    "np_2d_array = np.array([[1,2,3], \n",
    "                        [4,5,6]])\n",
    "print(np_2d_array)\n",
    "\n",
    "# Calculate the transpose, which is when you swap the columns and rows.\n",
    "print(\"--1--\")\n",
    "np_2d_array_T = np_2d_array.T\n",
    "print(np_2d_array_T)\n",
    "\n",
    "# Print the shape of the array as (number of rows, number of columns)\n",
    "print(\"--3--\")\n",
    "print(np_2d_array.shape)\n",
    "\n",
    "# Access elements in the 2d array by index. \n",
    "# First index is the row number\n",
    "# Second index is the column number\n",
    "# Index numbers start from 0\n",
    "print(\"--4--\")\n",
    "print(np_2d_array[1,1])\n",
    "print(np_2d_array[0,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling the data \n",
    "Once you have some data, it can be useful to sample from it.\n",
    "\n",
    "As a refresher, sampling is a way to take a smaller group from a population. For example, you might create a random sample from the U.S. population randomly knocking on 10 doors in the U.S. That would be a sample size of 10.\n",
    "\n",
    "The choice() function allows you to pass an array, specify how many values to sample, and decide whether sampling should be done with or without replacement. Sampling without replacement means the same value can’t be sampled more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0--\n",
      "[2 2 4 3 3 4 2 1 4 5]\n",
      "--1--\n",
      "[2 5 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.array([1,2,3,4,5])\n",
    "\n",
    "# Sample 10 data points with replacement. \n",
    "print(\"--0--\")\n",
    "print(np.random.choice(array, 10, replace=True))\n",
    "\n",
    "# Sample 3 data points without replacement. \n",
    "print(\"--1--\")\n",
    "print(np.random.choice(array, 3, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 5, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1,2,3,4,5]  # Create a list of 5 elements\n",
    "np.random.shuffle(x)  # Randomly shuffle the order of the elements in the list\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy an External Library\n",
    "This lesson introduces an external scipy library by discussing in detail how scipy provides support to handle statistics and probabilistic functionalities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating correlations \n",
    "Scipy is a Python library for scientific computing. Scipy and Numpy are the core libraries that Pandas is built upon. We will discuss Pandas later in the course, but having an understanding of Scipy and Numpy before discussing Pandas is useful.\n",
    "\n",
    "A correlation is a numerical measure of the statistical relationship between two variables. For us, those variables will usually be two columns of data, for example, the temperature outside and the likelihood of rain.\n",
    "\n",
    "One way to calculate the correlation between two vectors of data is with Pearson’s r-value. This value ranges between -1 and 1. Where -1 means there is a total negative correlation, 0 means no correlation, and 1 means total positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PearsonRResult(statistic=0.9999999999999999, pvalue=1.8488927466117464e-32)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "array_1 = np.array([1,2,3,4,5,6])  # Create a numpy array from a list\n",
    "array_2 = array_1  # Create another array with the same values\n",
    "\n",
    "print(stats.pearsonr(array_1, array_2))  # Calculate the correlation which will be 1 since the values are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.4080289    8.74384029  -0.97225648   3.18329968  16.18033781\n",
      " -11.89214579  -6.19223903  -4.18226201  -0.1299328    5.28423092]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "x = stats.norm.rvs(loc=0, scale=10, size=10)  # Generate 10 values randomly sampled from a normal distribution with mean 0 and standard deviation of 10\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.69459862670642e-24\n",
      "0.03989422804014327\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "p1 = stats.norm.pdf(x=-100, loc=0, scale=10)  # Get probability of sampling a value of -100\n",
    "p2 = stats.norm.pdf(x=0, loc=0, scale=10)     # Get probability of sampling a value of 0\n",
    "\n",
    "print(p1)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=500, minmax=(-2.8725880003088275, 2.8665288954726305), mean=0.023191363856166755, variance=1.014112664629023, skewness=-0.05153834576173098, kurtosis=-0.21087812976765408)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.describe(stats.norm.rvs(loc=0, scale=1, size=500)))  # Calculate descriptive statistics for 500 data points sampled from normal distribution with mean 0 and standard deviation of 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  educationnum  \\\n",
      "0   39          State-gov   77516   Bachelors            13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors            13   \n",
      "2   38            Private  215646     HS-grad             9   \n",
      "3   53            Private  234721        11th             7   \n",
      "4   28            Private  338409   Bachelors            13   \n",
      "\n",
      "         maritalstatus          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capitalgain  capitalloss  hoursperweek   nativecountry   label  \n",
      "0         2174            0            40   United-States   <=50K  \n",
      "1            0            0            13   United-States   <=50K  \n",
      "2            0            0            40   United-States   <=50K  \n",
      "3            0            0            40   United-States   <=50K  \n",
      "4            0            0            40            Cuba   <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names as a list\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',\n",
    "        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']\n",
    "# Read in the CSV file from the webpage using the defined column names\n",
    "df = pd.read_csv(\"adult.data\", header=None, names=names)\n",
    "                      \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to JSON file \n",
    "JSON (JavaScript Object Notation) is a popular format allowing for a more flexible schema. It is also easy for humans to read and write. A lot of the data sent around the web is transmitted as JSON. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'glossary': {'title': 'example glossary', 'GlossDiv': {'title': 'S', 'GlossList': {'GlossEntry': {'ID': 'SGML', 'SortAs': 'SGML', 'GlossTerm': 'Standard Generalized Markup Language', 'Acronym': 'SGML', 'Abbrev': 'ISO 8879:1986', 'GlossDef': {'para': 'A meta-markup language, used to create markup languages such as DocBook.', 'GlossSeeAlso': ['GML', 'XML']}, 'GlossSee': 'markup'}}}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "## Define the JSON object as a string\n",
    "json_string = \"\"\"{\n",
    "    \"glossary\": {\n",
    "        \"title\": \"example glossary\",\n",
    "        \"GlossDiv\": {\n",
    "            \"title\": \"S\",\n",
    "            \"GlossList\": {\n",
    "                \"GlossEntry\": {\n",
    "                    \"ID\": \"SGML\",\n",
    "                    \"SortAs\": \"SGML\",\n",
    "                    \"GlossTerm\": \"Standard Generalized Markup Language\",\n",
    "                    \"Acronym\": \"SGML\",\n",
    "                    \"Abbrev\": \"ISO 8879:1986\",\n",
    "                    \"GlossDef\": {\n",
    "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
    "                        \"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
    "                    },\n",
    "                    \"GlossSee\": \"markup\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "# Read the JSON data into Python\n",
    "json_data = json.loads(json_string)\n",
    "\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data.json') as f:\n",
    " #   data = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to raw files \n",
    "Sometimes you get data in strange formats and you have to roll your own Python code to process the data. Fortunately, doing this is simple.\n",
    "\n",
    "For this, we will assume that you have data in some type of text file. Each row of data corresponds to a row in your text file.\n",
    "\n",
    "For example, you might have a file delimited by a pipe (|). It could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###import tempfile\n",
    "\n",
    "#tmp = tempfile.NamedTemporaryFile()\n",
    "\n",
    "# Open the file for writing. And write the data.\n",
    "#with open(tmp.name, 'w') as f:\n",
    "   # f.write(\"James|22|M\\n\")\n",
    "  #  f.write(\"Sarah|31|F\\n\")\n",
    " #   f.write(\"Mindy|25|F\")\n",
    "\n",
    "# Read in the data from our file, line by line\n",
    "#with open(tmp.name, \"r\") as f:\n",
    "   # for line in f:\n",
    "        #             print(line)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  educationnum  \\\n",
      "0   39          State-gov   77516   Bachelors            13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors            13   \n",
      "2   38            Private  215646     HS-grad             9   \n",
      "3   53            Private  234721        11th             7   \n",
      "4   28            Private  338409   Bachelors            13   \n",
      "\n",
      "         maritalstatus          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capitalgain  capitalloss  hoursperweek   nativecountry   label  \n",
      "0         2174            0            40   United-States   <=50K  \n",
      "1            0            0            13   United-States   <=50K  \n",
      "2            0            0            40   United-States   <=50K  \n",
      "3            0            0            40   United-States   <=50K  \n",
      "4            0            0            40            Cuba   <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',\n",
    "        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']\n",
    "train_df = pd.read_csv(\"adult.data\", header=None, names=names)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                count           mean            std      min       25%  \\\n",
      "age           32561.0      38.581647      13.640433     17.0      28.0   \n",
      "fnlwgt        32561.0  189778.366512  105549.977697  12285.0  117827.0   \n",
      "educationnum  32561.0      10.080679       2.572720      1.0       9.0   \n",
      "capitalgain   32561.0    1077.648844    7385.292085      0.0       0.0   \n",
      "capitalloss   32561.0      87.303830     402.960219      0.0       0.0   \n",
      "hoursperweek  32561.0      40.437456      12.347429      1.0      40.0   \n",
      "\n",
      "                   50%       75%        max  \n",
      "age               37.0      48.0       90.0  \n",
      "fnlwgt        178356.0  237051.0  1484705.0  \n",
      "educationnum      10.0      12.0       16.0  \n",
      "capitalgain        0.0       0.0    99999.0  \n",
      "capitalloss        0.0       0.0     4356.0  \n",
      "hoursperweek      40.0      45.0       99.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',\n",
    "        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']\n",
    "train_df = pd.read_csv(\"adult.data\", header=None, names=names)\n",
    "print(train_df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   age            32561 non-null  int64 \n",
      " 1   workclass      32561 non-null  object\n",
      " 2   fnlwgt         32561 non-null  int64 \n",
      " 3   education      32561 non-null  object\n",
      " 4   educationnum   32561 non-null  int64 \n",
      " 5   maritalstatus  32561 non-null  object\n",
      " 6   occupation     32561 non-null  object\n",
      " 7   relationship   32561 non-null  object\n",
      " 8   race           32561 non-null  object\n",
      " 9   sex            32561 non-null  object\n",
      " 10  capitalgain    32561 non-null  int64 \n",
      " 11  capitalloss    32561 non-null  int64 \n",
      " 12  hoursperweek   32561 non-null  int64 \n",
      " 13  nativecountry  32561 non-null  object\n",
      " 14  label          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in data as explained in reading CSV lesson\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',\n",
    "        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']\n",
    "train_df = pd.read_csv(\"adult.data\", header=None, names=names)\n",
    "                      \n",
    "print(train_df.info())  # Use the info() function on the dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data types \n",
    "If a column doesn’t seem to have the correct type, it is easy to convert it to different types using .to_() functions:\n",
    "\n",
    "to_numeric()\n",
    "to_datetime()\n",
    "to_string()\n",
    "For example:\n",
    "\n",
    "df['numeric_column'] = pd.to_numeric(df['string_column'])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Not-in-family' ' Husband' ' Wife' ' Own-child' ' Unmarried'\n",
      " ' Other-relative']\n"
     ]
    }
   ],
   "source": [
    "# check for the unique value \n",
    "print(train_df['relationship'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Husband           13193\n",
      " Not-in-family      8305\n",
      " Own-child          5068\n",
      " Unmarried          3446\n",
      " Wife               1568\n",
      " Other-relative      981\n",
      "Name: relationship, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',\n",
    "        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']\n",
    "train_df = pd.read_csv(\"adult.data\", header=None, names=names)\n",
    "print(train_df['relationship'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping the data \n",
    "We can also do these types of counts by specific groups by using the groupby() function. This function takes a list of columns by which you would like to group your dataframe. It then performs the requested calculations on each group individually and returns the results by group. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationship     label \n",
      " Husband          <=50K    0.551429\n",
      "                  >50K     0.448571\n",
      " Not-in-family    <=50K    0.896930\n",
      "                  >50K     0.103070\n",
      " Other-relative   <=50K    0.962283\n",
      "                  >50K     0.037717\n",
      " Own-child        <=50K    0.986780\n",
      "                  >50K     0.013220\n",
      " Unmarried        <=50K    0.936738\n",
      "                  >50K     0.063262\n",
      " Wife             <=50K    0.524872\n",
      "                  >50K     0.475128\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',\n",
    "        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']\n",
    "train_df = pd.read_csv(\"adult.data\", header=None, names=names)\n",
    "\n",
    "# Group by relationship and then get the value counts of label with normalization                   \n",
    "print(train_df.groupby('relationship')['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass\n",
      " ?                   31.919390\n",
      " Federal-gov         41.379167\n",
      " Local-gov           40.982800\n",
      " Never-worked        28.428571\n",
      " Private             40.267096\n",
      " Self-emp-inc        48.818100\n",
      " Self-emp-not-inc    44.421881\n",
      " State-gov           39.031587\n",
      " Without-pay         32.714286\n",
      "Name: hoursperweek, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_df.groupby(['workclass'])['hoursperweek'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding the correlation \n",
    "Another useful statistic is the correlation. If you need a refresher on correlation, please check out Wikipedia. You can calculate all the pair-wise correlations in your dataframe by using the corr function.\n",
    "\n",
    "1234567\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANZAR AZIZ\\AppData\\Local\\Temp\\ipykernel_6312\\238115825.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  print(train_df.corr())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   age    fnlwgt  educationnum  capitalgain  capitalloss  \\\n",
      "age           1.000000 -0.076646      0.036527     0.077674     0.057775   \n",
      "fnlwgt       -0.076646  1.000000     -0.043195     0.000432    -0.010252   \n",
      "educationnum  0.036527 -0.043195      1.000000     0.122630     0.079923   \n",
      "capitalgain   0.077674  0.000432      0.122630     1.000000    -0.031615   \n",
      "capitalloss   0.057775 -0.010252      0.079923    -0.031615     1.000000   \n",
      "hoursperweek  0.068756 -0.018768      0.148123     0.078409     0.054256   \n",
      "\n",
      "              hoursperweek  \n",
      "age               0.068756  \n",
      "fnlwgt           -0.018768  \n",
      "educationnum      0.148123  \n",
      "capitalgain       0.078409  \n",
      "capitalloss       0.054256  \n",
      "hoursperweek      1.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate correlations                   \n",
    "print(train_df.corr())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can quickly see that compared to all of the correlations, there is a higher correlation between “hours per week” and “education num”, but it is not very high. You will notice, though since our label is an object, it isn’t included here. Knowing how variables correlate with our label would be useful, so let’s take care of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANZAR AZIZ\\AppData\\Local\\Temp\\ipykernel_6312\\3559137043.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  print(train_df.corr())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   age    fnlwgt  educationnum  capitalgain  capitalloss  \\\n",
      "age           1.000000 -0.076646      0.036527     0.077674     0.057775   \n",
      "fnlwgt       -0.076646  1.000000     -0.043195     0.000432    -0.010252   \n",
      "educationnum  0.036527 -0.043195      1.000000     0.122630     0.079923   \n",
      "capitalgain   0.077674  0.000432      0.122630     1.000000    -0.031615   \n",
      "capitalloss   0.057775 -0.010252      0.079923    -0.031615     1.000000   \n",
      "hoursperweek  0.068756 -0.018768      0.148123     0.078409     0.054256   \n",
      "label_int     0.234037 -0.009463      0.335154     0.223329     0.150526   \n",
      "\n",
      "              hoursperweek  label_int  \n",
      "age               0.068756   0.234037  \n",
      "fnlwgt           -0.018768  -0.009463  \n",
      "educationnum      0.148123   0.335154  \n",
      "capitalgain       0.078409   0.223329  \n",
      "capitalloss       0.054256   0.150526  \n",
      "hoursperweek      1.000000   0.229689  \n",
      "label_int         0.229689   1.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the string label into a value of 1 when >= 50k and 0 otherwise\n",
    "train_df['label_int'] = train_df.label.apply(lambda x: \">\" in x)\n",
    "print(train_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age        fnlwgt  educationnum   capitalgain   capitalloss  \\\n",
      "count  32561.000000  3.256100e+04  32561.000000  32561.000000  32561.000000   \n",
      "mean      38.581647  1.897784e+05     10.080679   1077.648844     87.303830   \n",
      "std       13.640433  1.055500e+05      2.572720   7385.292085    402.960219   \n",
      "min       17.000000  1.228500e+04      1.000000      0.000000      0.000000   \n",
      "1%        17.000000  2.718580e+04      3.000000      0.000000      0.000000   \n",
      "5%        19.000000  3.946000e+04      5.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.783560e+05     10.000000      0.000000      0.000000   \n",
      "95%       63.000000  3.796820e+05     14.000000   5013.000000      0.000000   \n",
      "99%       74.000000  5.100720e+05     16.000000  15024.000000   1980.000000   \n",
      "max       90.000000  1.484705e+06     16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hoursperweek  \n",
      "count  32561.000000  \n",
      "mean      40.437456  \n",
      "std       12.347429  \n",
      "min        1.000000  \n",
      "1%         8.000000  \n",
      "5%        18.000000  \n",
      "50%       40.000000  \n",
      "95%       60.000000  \n",
      "99%       80.000000  \n",
      "max       99.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the describe function to calculate the percentiles specified                     \n",
    "print(train_df.describe(percentiles=[.01,.05,.95,.99]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " percentile is the value below which a given percent of the data falls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Table \n",
    "Somewhat like Excel, we can pivot our data using pandas pivot_table functionality. To do so, we will use the pivot_table() function.\n",
    "\n",
    "The values parameter is the column being used for aggregation, the index parameter is for the index values that creates multiple rows, and the columns parameter is for the value on which you want to have multiple columns created.\n",
    "\n",
    "You can also use the aggfunc parameter to pass a function with which to aggregate your pivots.\n",
    "\n",
    "Let’s look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label                               <=50K   >50K\n",
      "relationship    workclass                       \n",
      " Husband         ?                  30.72  37.33\n",
      "                 Federal-gov        42.34  43.05\n",
      "                 Local-gov          41.40  44.56\n",
      "                 Private            42.50  46.18\n",
      "                 Self-emp-inc       48.29  50.49\n",
      "                 Self-emp-not-inc   46.01  48.07\n",
      "                 State-gov          38.67  45.17\n",
      "                 Without-pay        34.25    NaN\n",
      " Not-in-family   ?                  31.29  39.44\n",
      "                 Federal-gov        40.60  47.54\n",
      "                 Local-gov          40.38  45.01\n",
      "                 Never-worked       35.00    NaN\n",
      "                 Private            40.20  47.03\n",
      "                 Self-emp-inc       49.06  53.58\n",
      "                 Self-emp-not-inc   41.53  45.02\n",
      "                 State-gov          38.87  44.19\n",
      " Other-relative  ?                  29.10  40.00\n",
      "                 Federal-gov        38.40  45.00\n",
      "                 Local-gov          35.92  48.00\n",
      "                 Private            37.44  40.74\n",
      "                 Self-emp-inc       40.00  41.67\n",
      "                 Self-emp-not-inc   36.16  49.29\n",
      "                 State-gov          36.40  29.00\n",
      " Own-child       ?                  32.39  50.00\n",
      "                 Federal-gov        35.11    NaN\n",
      "                 Local-gov          35.59  41.25\n",
      "                 Never-worked       24.80    NaN\n",
      "                 Private            32.84  43.09\n",
      "                 Self-emp-inc       39.60  43.75\n",
      "                 Self-emp-not-inc   40.33  49.38\n",
      "                 State-gov          30.10  38.33\n",
      "                 Without-pay        35.00    NaN\n",
      " Unmarried       ?                  32.75  50.00\n",
      "                 Federal-gov        39.30  43.65\n",
      "                 Local-gov          40.09  45.79\n",
      "                 Private            38.64  45.70\n",
      "                 Self-emp-inc       45.74  58.11\n",
      "                 Self-emp-not-inc   40.62  47.81\n",
      "                 State-gov          38.15  44.56\n",
      "                 Without-pay        37.50    NaN\n",
      " Wife            ?                  28.29  29.72\n",
      "                 Federal-gov        38.93  39.74\n",
      "                 Local-gov          37.87  40.38\n",
      "                 Never-worked       40.00    NaN\n",
      "                 Private            36.56  38.31\n",
      "                 Self-emp-inc       44.67  38.14\n",
      "                 Self-emp-not-inc   36.53  34.61\n",
      "                 State-gov          36.50  39.10\n",
      "                 Without-pay        23.67    NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Pivot the data frame to show by relationship, workclass (rows) and label (columns) the average hours per week.\n",
    "print(pd.pivot_table(train_df, values='hoursperweek', index=['relationship','workclass'], \n",
    "               columns=['label'], aggfunc=np.mean).round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Tab \n",
    "Crosstab is a nice way to get frequency tables. What you do is pass two columns to the function and you will get the frequency of all the pair-wise combinations of those two variables.\n",
    "\n",
    "Let’s look at an example using label and relationship as our columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationship   Husband   Not-in-family   Other-relative   Own-child  \\\n",
      "label                                                                 \n",
      " <=50K            7275            7449              944        5001   \n",
      " >50K             5918             856               37          67   \n",
      "\n",
      "relationship   Unmarried   Wife  \n",
      "label                            \n",
      " <=50K              3228    823  \n",
      " >50K                218    745  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate the frequencies between label and relationship\n",
    "print(pd.crosstab(train_df['label'], train_df.relationship))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationship   Husband   Not-in-family   Other-relative   Own-child  \\\n",
      "label                                                                 \n",
      " <=50K        0.223427        0.228771         0.028992    0.153589   \n",
      " >50K         0.181751        0.026289         0.001136    0.002058   \n",
      "\n",
      "relationship   Unmarried      Wife  \n",
      "label                               \n",
      " <=50K          0.099137  0.025276  \n",
      " >50K           0.006695  0.022880  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crosstab with normalized outputs\n",
    "print(pd.crosstab(train_df['label'], train_df.relationship, normalize=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape \n",
    "With Pandas, you can use pivot() to reshape your data. To illustrate this concept, I will use code from this post to create a dataframe in a long format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANZAR AZIZ\\AppData\\Local\\Temp\\ipykernel_6312\\304379548.py:1: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm; tm.N = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date variable     value\n",
      "0   2000-01-03        A  1.347667\n",
      "1   2000-01-04        A  0.722874\n",
      "2   2000-01-05        A -0.704584\n",
      "3   2000-01-06        A  0.337961\n",
      "4   2000-01-07        A -0.590463\n",
      "..         ...      ...       ...\n",
      "115 2000-02-07        D -0.004090\n",
      "116 2000-02-08        D -0.985388\n",
      "117 2000-02-09        D  1.226312\n",
      "118 2000-02-10        D  0.532080\n",
      "119 2000-02-11        D  1.556491\n",
      "\n",
      "[120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas.util.testing as tm; tm.N = 3\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "def unpivot(frame):\n",
    "    N, K = frame.shape\n",
    "    data = {'value' : frame.values.ravel('F'),\n",
    "            'variable' : np.asarray(frame.columns).repeat(N),\n",
    "            'date' : np.tile(np.asarray(frame.index), K)}\n",
    "    return pd.DataFrame(data, columns=['date', 'variable', 'value'])\n",
    "df = unpivot(tm.makeTimeDataFrame())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable           A         B         C         D\n",
      "date                                              \n",
      "2000-01-03  0.145465  1.275622 -0.796562  0.028705\n",
      "2000-01-04  1.842369  1.057237 -0.357448  0.411317\n",
      "2000-01-05  1.491366 -0.371615  0.935339  0.207586\n",
      "2000-01-06  0.502395 -1.210836 -1.726363  1.547884\n",
      "2000-01-07 -0.970034 -1.122811 -1.392785 -0.064032\n",
      "2000-01-10  0.676547 -0.930433  0.082083 -0.398931\n",
      "2000-01-11  0.708579  0.442614  0.245391 -1.182064\n",
      "2000-01-12 -0.357984 -0.884836 -0.845117 -1.115405\n",
      "2000-01-13  0.022101  1.412799  0.594949  0.574728\n",
      "2000-01-14 -0.467741 -0.427137  0.927292  0.017947\n",
      "2000-01-17  0.579834 -1.066233  3.110033 -1.071741\n",
      "2000-01-18  0.274805 -0.560597  0.927202 -2.048019\n",
      "2000-01-19 -1.514235  0.319917 -0.988648  0.280133\n",
      "2000-01-20  1.625991 -2.357232 -0.356871 -0.954610\n",
      "2000-01-21 -0.093331  0.196612  0.889723 -0.497676\n",
      "2000-01-24 -0.286103  0.244241  0.476519 -0.106592\n",
      "2000-01-25  0.152012 -0.399965  0.772984  0.707590\n",
      "2000-01-26 -0.305735 -0.916677  1.221145 -1.614432\n",
      "2000-01-27  2.211329 -0.365260  0.031915  1.179297\n",
      "2000-01-28 -1.260762 -0.914961 -0.936342 -0.245065\n",
      "2000-01-31 -0.234921  0.423814  0.276771 -0.807446\n",
      "2000-02-01  0.643658  0.029208 -0.025702 -1.489590\n",
      "2000-02-02 -0.959640 -0.638773 -0.869349 -0.958879\n",
      "2000-02-03  1.111314  0.932472  0.414671  0.213144\n",
      "2000-02-04 -0.490266  0.068394  0.988167 -0.499929\n",
      "2000-02-07 -0.513592 -0.372952  0.669628  0.337265\n",
      "2000-02-08 -0.341012 -0.911893  2.066787 -1.335387\n",
      "2000-02-09  0.279433  2.321143 -0.168930 -1.833061\n",
      "2000-02-10 -1.755169 -1.375460  0.618592 -0.165587\n",
      "2000-02-11 -0.865819 -0.197605 -0.064750 -0.456152\n"
     ]
    }
   ],
   "source": [
    "import pandas.util.testing as tm; tm.N = 3\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "def unpivot(frame):\n",
    "    N, K = frame.shape\n",
    "    data = {'value' : frame.values.ravel('F'),\n",
    "            'variable' : np.asarray(frame.columns).repeat(N),\n",
    "            'date' : np.tile(np.asarray(frame.index), K)}\n",
    "    return pd.DataFrame(data, columns=['date', 'variable', 'value'])\n",
    "df = unpivot(tm.makeTimeDataFrame())\n",
    "\n",
    "# Use pivot to keep date as the index and value as the values, but use the vaiable column to create new columns\n",
    "df_pivot = df.pivot(index='date', columns='variable', values='value')\n",
    "print(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable  date      \n",
      "A         2000-01-03    0.492193\n",
      "          2000-01-04   -0.363407\n",
      "          2000-01-05   -0.345795\n",
      "          2000-01-06    1.137429\n",
      "          2000-01-07    0.303698\n",
      "                          ...   \n",
      "D         2000-02-07   -0.540596\n",
      "          2000-02-08    0.631036\n",
      "          2000-02-09    1.373140\n",
      "          2000-02-10   -0.945360\n",
      "          2000-02-11   -0.391222\n",
      "Length: 120, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas.util.testing as tm; tm.N = 3\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Create long dataframe\n",
    "def unpivot(frame):\n",
    "    N, K = frame.shape\n",
    "    data = {'value' : frame.values.ravel('F'),\n",
    "            'variable' : np.asarray(frame.columns).repeat(N),\n",
    "            'date' : np.tile(np.asarray(frame.index), K)}\n",
    "    return pd.DataFrame(data, columns=['date', 'variable', 'value'])\n",
    "df = unpivot(tm.makeTimeDataFrame())\n",
    "\n",
    "# Convert to wide format\n",
    "df_pivot = df.pivot(index='date', columns='variable', values='value')\n",
    "\n",
    "# Convert back to long format\n",
    "print(df_pivot.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'variable', 'value'], dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Loading dataset\n",
    "# def read_csv():\n",
    "#     # Define the column names as a list\n",
    "    \n",
    "#     # Read in the CSV file from the webpage using the defined column names\n",
    "#     df = pd.read_csv(\"adult.data\", header=None, names=names, delim_whitespace=True)\n",
    "#     return df\n",
    "\n",
    "# # Describing data\n",
    "# def group_aggregation(df, group_var, agg_var):\n",
    "\n",
    "#     # Grouping the data and taking mean\n",
    "#     grouped_df = df.groupby([group_var])[agg_var].mean()\n",
    "#     return grouped_df\n",
    "\n",
    "# # Calling the function\n",
    "# print(group_aggregation(read_csv(),\"capitalgain\",\"hoursperweek\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of scaling \n",
    "Standard scaling \n",
    "Standard scaling subtracts the mean and divides by the standard deviation. This centers the feature on zero with unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Standard scaling\n",
      "[-0.125  9.   ]\n",
      "[0.73950997 5.91607978]\n",
      "After Standard scaling\n",
      "[0. 0.]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a matrix of data\n",
    "data = [[-1, 2], \n",
    "        [-0.5, 6], \n",
    "        [0, 10], \n",
    "        [1, 18]]\n",
    "\n",
    "print(\"Before Standard scaling\")\n",
    "print(np.mean(data, 0))\n",
    "print(np.std(data, 0))\n",
    "\n",
    "# Initalize a StandardScaler\n",
    "standard = StandardScaler()\n",
    "# Fit and transform the data with the StandardScaler\n",
    "standard_data = standard.fit_transform(data)\n",
    "\n",
    "print(\"After Standard scaling\")\n",
    "print(np.mean(standard_data, 0))\n",
    "print(np.std(standard_data, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we created a NumPy array of shape (4,2). We then use the StandardScaler() from sklearn which will automatically subtract the mean and divide by the standard deviation of each of our columns. This is done with the fit_transform() call.\n",
    "\n",
    "We check that it worked by printing the mean and standard deviation. We can see that both columns now have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min/Max scaling \n",
    "Let’s look at the same example but instead use the MinMaxScaler() from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[1. 1.]\n",
      "[0.4375 0.4375]\n",
      "[0.36975499 0.36975499]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create matrix of data\n",
    "data = [[-1, 2], \n",
    "        [-0.5, 6], \n",
    "        [0, 10], \n",
    "        [1, 18]]\n",
    "\n",
    "# Initalize MinMaxScaler\n",
    "min_max = MinMaxScaler()\n",
    "# Fit and transform the data\n",
    "min_max_data = min_max.fit_transform(data)\n",
    "\n",
    "print(np.min(min_max_data, 0))\n",
    "print(np.max(min_max_data, 0))\n",
    "print(np.mean(min_max_data, 0))\n",
    "print(np.std(min_max_data, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to categorical data \n",
    "Sometimes you get categorical data which are variables with a limited and usually fixed number of values. For example, male and female. Machine learning algorithms need numbers to work, so how do you deal with these? We will discuss two ways:\n",
    "\n",
    "Label encoding\n",
    "\n",
    "One-hot encoding a.k.a. dummy variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with categorical data \n",
    "Label encoding\n",
    " \n",
    "Label encoding works by converting the unique values to a numeric representation. For example, if we have two categories male and female, we can categorize them as numbers:\n",
    "\n",
    "male as 0\n",
    "female 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "dtype: int8\n",
      "Index(['female', 'male'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create series with male and female values\n",
    "non_categorical_series = pd.Series(['male', 'female', 'male', 'female'])\n",
    "# Convert the text series to a categorical series\n",
    "categorical_series = non_categorical_series.astype('category')\n",
    "# Print the numeric codes for each value\n",
    "print(categorical_series.cat.codes)\n",
    "# Print the category names\n",
    "print(categorical_series.cat.categories)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding \n",
    "\n",
    "One-hot encoding is similar but creates a new column for each category and fills it with a 1 for each row with that value and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   female  male\n",
      "0       0     1\n",
      "1       1     0\n",
      "2       0     1\n",
      "3       1     0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create series with male and female values\n",
    "non_categorical_series = pd.Series(['male', 'female', 'male', 'female'])\n",
    "# Create dummy or one-hot encoded variables\n",
    "print(pd.get_dummies(non_categorical_series))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we just had to use the get_dummies() call on our series and it automatically makes new columns for each unique value in our series and fills them with a 1 or 0 as appropriate.\n",
    "\n",
    "Now that you have a grasp on some ways of cleaning our data, the next lesson brings you a challenge to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def read_csv():\n",
    "#     # Define the column names as a list\n",
    "#     names = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\", \"origin\", \"car_name\"]\n",
    "#     # Read in the CSV file from the webpage using the defined column names\n",
    "#     df = pd.read_csv(\"auto-mpg.data\", header=None, names=names, delim_whitespace=True)\n",
    "#     return df\n",
    "\n",
    "# # Remving outliers from the data\n",
    "# def outlier_detection(df):\n",
    "#     df = df.quantile([.90, .10])\n",
    "#     return df\n",
    "\n",
    "# print(outlier_detection(read_csv()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of data visualization \n",
    "So far, we have looked at understanding data via descriptive statistics and tables. Another useful tool is visualization.\n",
    "\n",
    "Visualizations of data can provide the following benefits:\n",
    "\n",
    "A better understanding of the data\n",
    "\n",
    "A more compelling story when explaining the data\n",
    "\n",
    "An easier to comprehend medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the boston dataset from sklearn.datasets\n",
    "# boston_data = load_boston()\n",
    "# # Enter the boston data into a dataframe\n",
    "# boston_df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)\n",
    "\n",
    "# # Print the first 5 rows to confirm ran correctly\n",
    "# # print(boston_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
